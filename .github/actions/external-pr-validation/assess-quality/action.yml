name: 'External Assess Quality'
description: 'Provide friendly, focused AI feedback on code quality and tests'
inputs:
  github-token:
    description: 'GitHub token for API access'
    required: true
  pr-number:
    description: 'PR number to assess'
    required: true
  pr-type:
    description: 'PR type: docs, bugfix, or feature'
    required: true
  pr-size:
    description: 'PR size: small, medium, or large'
    required: true
  openai-api-key:
    description: 'OpenAI API key (reuse existing)'
    required: false
    default: ''
outputs:
  feedback-text:
    description: 'Friendly feedback formatted for comment'
    value: ${{ steps.assess.outputs.feedback-text }}
  has-concerns:
    description: 'Boolean - true only if critical issues found'
    value: ${{ steps.assess.outputs.has-concerns }}
  summary:
    description: 'One-sentence assessment'
    value: ${{ steps.assess.outputs.summary }}

runs:
  using: 'composite'
  steps:
    - name: Check if API key is available
      id: check-key
      shell: bash
      env:
        API_KEY: ${{ inputs.openai-api-key }}
      run: |
        if [ -z "$API_KEY" ]; then
          echo "has-key=false" >> "$GITHUB_OUTPUT"
          echo "WARNING: OpenAI API key not configured - skipping AI assessment"
        else
          echo "has-key=true" >> "$GITHUB_OUTPUT"
          echo "OpenAI API key available - running AI assessment"
        fi

    - name: Get PR content
      id: get-pr
      if: steps.check-key.outputs.has-key == 'true'
      shell: bash
      env:
        GH_TOKEN: ${{ inputs.github-token }}
        PR_NUMBER: ${{ inputs.pr-number }}
      run: |
        # Get PR diff and metadata
        echo "Fetching PR content..."
        
        # Get PR diff (limited to prevent huge diffs)
        gh pr diff "$PR_NUMBER" | head -c 50000 > pr_diff.txt
        diff_content=$(cat pr_diff.txt)
        
        # Get changed files list
        files=$(gh pr view "$PR_NUMBER" --json files --jq '.files[].path' | head -n 50 | tr '\n' ', ')
        
        # Get PR title and body
        pr_title=$(gh pr view "$PR_NUMBER" --json title --jq '.title')
        pr_body=$(gh pr view "$PR_NUMBER" --json body --jq '.body' | head -c 2000)
        
        echo "pr-title=$pr_title" >> "$GITHUB_OUTPUT"
        echo "files=$files" >> "$GITHUB_OUTPUT"
        
        echo "pr-body<<EOF" >> "$GITHUB_OUTPUT"
        echo "$pr_body" >> "$GITHUB_OUTPUT"
        echo "EOF" >> "$GITHUB_OUTPUT"
        
        echo "diff<<EOF" >> "$GITHUB_OUTPUT"
        echo "$diff_content" >> "$GITHUB_OUTPUT"
        echo "EOF" >> "$GITHUB_OUTPUT"

    - name: Run AI assessment
      id: assess
      shell: bash
      env:
        HAS_KEY: ${{ steps.check-key.outputs.has-key }}
        OPENAI_API_KEY: ${{ inputs.openai-api-key }}
        PR_TYPE: ${{ inputs.pr-type }}
        PR_SIZE: ${{ inputs.pr-size }}
        PR_TITLE: ${{ steps.get-pr.outputs.pr-title }}
        PR_BODY: ${{ steps.get-pr.outputs.pr-body }}
        FILES: ${{ steps.get-pr.outputs.files }}
        DIFF: ${{ steps.get-pr.outputs.diff }}
      run: |
        if [ "$HAS_KEY" = "false" ]; then
          # No API key - provide friendly fallback message
          echo "feedback-text=*AI assessment skipped (API key not configured). Human reviewers will check code quality and tests.*" >> "$GITHUB_OUTPUT"
          echo "has-concerns=false" >> "$GITHUB_OUTPUT"
          echo "summary=AI assessment unavailable" >> "$GITHUB_OUTPUT"
          exit 0
        fi
        
        echo "Running AI quality assessment for $PR_TYPE PR (size: $PR_SIZE)..."
        
        # Build type-specific system prompt
        case "$PR_TYPE" in
          docs)
            FOCUS="You're providing an initial quick review of this documentation PR. Automated checks are still running. Focus on: obvious content issues, structural problems, or critical gaps. Be helpful and fair. Keep it to 1-2 sentences. If there's nothing critical to mention, respond with just 'Automated checks are running. We'll review once they complete.' Don't create expectations about merging - this is just an initial scan."
            ;;
          bugfix)
            FOCUS="You're providing an initial quick review of this bugfix PR. Automated checks are still running. Focus on: obvious missing test coverage for the fix, potential new bugs in the changes, or scope creep. Be helpful and fair. Keep it to 1-2 sentences. If there's nothing critical to mention, respond with just 'Automated checks are running. We'll review once they complete.' Don't create expectations about merging - this is just an initial scan."
            ;;
          feature)
            FOCUS="You're providing an initial quick review of this feature PR. Automated checks are still running. Focus on: obvious missing test coverage, potential bugs or edge cases not handled, or code quality concerns. Be helpful and fair. Keep it to 1-2 sentences. If there's nothing critical to mention, respond with just 'Automated checks are running. We'll review once they complete.' Don't create expectations about merging - this is just an initial scan."
            ;;
        esac
        
        # Add size-specific guidance
        if [ "$PR_SIZE" = "small" ]; then
          FOCUS="$FOCUS This is a small PR - be brief."
        elif [ "$PR_SIZE" = "medium" ]; then
          FOCUS="$FOCUS This is a medium PR - focus on the most important issues."
        else
          FOCUS="$FOCUS This is a large PR (post-alignment) - only mention critical blockers."
        fi
        
        # Build user message with PR context
        USER_MSG=$(printf "Title: %s\n\nFiles changed: %s\n\nPR Description:\n%s\n\nCode changes:\n%s\n\nProvide a quick initial review. Remember: automated checks are still running, so most checkboxes will be unchecked. Only mention critical issues you can spot in the code. If nothing critical, just say automated checks are running." "$PR_TITLE" "$FILES" "$PR_BODY" "$DIFF")
        
        # Build JSON payload using jq
        payload=$(jq -n \
          --arg model "gpt-4o-mini" \
          --arg system_prompt "$FOCUS" \
          --arg user_msg "$USER_MSG" \
          '{"model":$model,"messages":[{"role":"system","content":$system_prompt},{"role":"user","content":$user_msg}],"temperature":0.3,"max_tokens":300}')
        
        # Call OpenAI API
        response=$(curl -s -X POST https://api.openai.com/v1/chat/completions \
          -H "Content-Type: application/json" \
          -H "Authorization: Bearer $OPENAI_API_KEY" \
          -d "$payload")
        
        # Check for API errors
        api_error=$(echo "$response" | jq -r '.error.message // ""')
        if [ -n "$api_error" ]; then
          echo "WARNING: OpenAI API error: $api_error"
          echo "feedback-text=*AI assessment encountered an error. Human reviewers will check code quality and tests.*" >> "$GITHUB_OUTPUT"
          echo "has-concerns=false" >> "$GITHUB_OUTPUT"
          echo "summary=AI assessment error" >> "$GITHUB_OUTPUT"
          exit 0
        fi
        
        # Extract feedback
        feedback=$(echo "$response" | jq -r '.choices[0].message.content // ""')
        
        if [ -z "$feedback" ]; then
          echo "WARNING: Empty response from AI"
          echo "feedback-text=*AI assessment returned no feedback. Human reviewers will check code quality and tests.*" >> "$GITHUB_OUTPUT"
          echo "has-concerns=false" >> "$GITHUB_OUTPUT"
          echo "summary=AI assessment empty" >> "$GITHUB_OUTPUT"
          exit 0
        fi
        
        echo "AI assessment complete"
        
        # Detect if there are concerns (look for warning keywords)
        has_concerns=false
        if echo "$feedback" | grep -qiE "(concern|issue|problem|bug|missing|should|must|need to)"; then
          has_concerns=true
        fi
        
        # Create summary (first sentence)
        summary=$(echo "$feedback" | head -n 1 | cut -d'.' -f1)
        
        echo "feedback-text=$feedback" >> "$GITHUB_OUTPUT"
        echo "has-concerns=$has_concerns" >> "$GITHUB_OUTPUT"
        echo "summary=$summary" >> "$GITHUB_OUTPUT"
        
        echo "Assessment summary: $summary"

