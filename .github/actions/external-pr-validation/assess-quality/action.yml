name: 'External Assess Quality'
description: 'Provide friendly, focused AI feedback on code quality and tests'
inputs:
  github-token:
    description: 'GitHub token for API access'
    required: true
  pr-number:
    description: 'PR number to assess'
    required: true
  pr-type:
    description: 'PR type: docs, bugfix, or feature'
    required: true
  pr-size:
    description: 'PR size: small, medium, or large'
    required: true
  openai-api-key:
    description: 'OpenAI API key (reuse existing)'
    required: false
    default: ''
outputs:
  feedback-text:
    description: 'Friendly feedback formatted for comment'
    value: ${{ steps.assess.outputs.feedback-text }}
  has-concerns:
    description: 'Boolean - true only if critical issues found'
    value: ${{ steps.assess.outputs.has-concerns }}
  summary:
    description: 'One-sentence assessment'
    value: ${{ steps.assess.outputs.summary }}

runs:
  using: 'composite'
  steps:
    - name: Check if API key is available
      id: check-key
      shell: bash
      env:
        API_KEY: ${{ inputs.openai-api-key }}
      run: |
        if [ -z "$API_KEY" ]; then
          echo "has-key=false" >> "$GITHUB_OUTPUT"
          echo "⚠️ OpenAI API key not configured - skipping AI assessment"
        else
          echo "has-key=true" >> "$GITHUB_OUTPUT"
          echo "✅ OpenAI API key available - running AI assessment"
        fi

    - name: Get PR content
      id: get-pr
      if: steps.check-key.outputs.has-key == 'true'
      shell: bash
      env:
        GH_TOKEN: ${{ inputs.github-token }}
        PR_NUMBER: ${{ inputs.pr-number }}
      run: |
        # Get PR diff and metadata
        echo "Fetching PR content..."
        
        # Get PR diff (limited to prevent huge diffs)
        gh pr diff "$PR_NUMBER" | head -c 50000 > pr_diff.txt
        diff_content=$(cat pr_diff.txt)
        
        # Get changed files list
        files=$(gh pr view "$PR_NUMBER" --json files --jq '.files[].path' | head -n 50 | tr '\n' ', ')
        
        # Get PR title and body
        pr_title=$(gh pr view "$PR_NUMBER" --json title --jq '.title')
        pr_body=$(gh pr view "$PR_NUMBER" --json body --jq '.body' | head -c 2000)
        
        echo "pr-title=$pr_title" >> "$GITHUB_OUTPUT"
        echo "files=$files" >> "$GITHUB_OUTPUT"
        
        echo "pr-body<<EOF" >> "$GITHUB_OUTPUT"
        echo "$pr_body" >> "$GITHUB_OUTPUT"
        echo "EOF" >> "$GITHUB_OUTPUT"
        
        echo "diff<<EOF" >> "$GITHUB_OUTPUT"
        echo "$diff_content" >> "$GITHUB_OUTPUT"
        echo "EOF" >> "$GITHUB_OUTPUT"

    - name: Run AI assessment
      id: assess
      shell: bash
      env:
        HAS_KEY: ${{ steps.check-key.outputs.has-key }}
        OPENAI_API_KEY: ${{ inputs.openai-api-key }}
        PR_TYPE: ${{ inputs.pr-type }}
        PR_SIZE: ${{ inputs.pr-size }}
        PR_TITLE: ${{ steps.get-pr.outputs.pr-title }}
        PR_BODY: ${{ steps.get-pr.outputs.pr-body }}
        FILES: ${{ steps.get-pr.outputs.files }}
        DIFF: ${{ steps.get-pr.outputs.diff }}
      run: |
        if [ "$HAS_KEY" = "false" ]; then
          # No API key - provide friendly fallback message
          echo "feedback-text=*AI assessment skipped (API key not configured). Human reviewers will check code quality and tests.*" >> "$GITHUB_OUTPUT"
          echo "has-concerns=false" >> "$GITHUB_OUTPUT"
          echo "summary=AI assessment unavailable" >> "$GITHUB_OUTPUT"
          exit 0
        fi
        
        echo "Running AI quality assessment for $PR_TYPE PR (size: $PR_SIZE)..."
        
        # Build type-specific system prompt
        case "$PR_TYPE" in
          docs)
            FOCUS="Review this documentation PR. Focus on: content clarity and helpfulness, appropriate structure for the size. Be encouraging and positive. Keep feedback to 2-3 sentences max. Only mention critical issues, ignore minor formatting. Always start with something positive."
            ;;
          bugfix)
            FOCUS="Review this bugfix PR. Focus on: whether tests actually verify the fix, any obvious new bugs introduced, if the fix is focused (not scope creep). Be encouraging and positive. Keep feedback to 2-3 sentences max. Only mention critical issues, ignore cosmetic stuff. Always start with something positive."
            ;;
          feature)
            FOCUS="Review this feature PR. Focus on: whether tests cover the new functionality adequately, code quality and maintainability, any obvious bugs or missed edge cases. Be encouraging and positive. Keep feedback to 2-3 sentences max. Only mention critical issues, ignore cosmetic stuff. Always start with something positive."
            ;;
        esac
        
        # Add size-specific guidance
        if [ "$PR_SIZE" = "small" ]; then
          FOCUS="$FOCUS This is a SMALL PR, so keep review lightweight and quick."
        elif [ "$PR_SIZE" = "medium" ]; then
          FOCUS="$FOCUS This is a MEDIUM PR, so be thorough but concise."
        else
          FOCUS="$FOCUS This is a LARGE PR (post-alignment), so focus on critical issues only."
        fi
        
        # Build user message with PR context using heredoc
        read -r -d '' USER_MSG << ENDOFMSG || true
Title: $PR_TITLE

Files changed: $FILES

PR Description:
$PR_BODY

Code changes:
$DIFF

Please provide friendly, concise feedback focused on code quality and tests.
ENDOFMSG
        
        # Build JSON payload using jq
        payload=$(jq -n \
          --arg model "gpt-4o-mini" \
          --arg system_prompt "$FOCUS" \
          --arg user_msg "$USER_MSG" \
          '{
            model: $model,
            messages: [
              {role: "system", content: $system_prompt},
              {role: "user", content: $user_msg}
            ],
            temperature: 0.3,
            max_tokens: 300
          }')
        
        # Call OpenAI API
        response=$(curl -s -X POST https://api.openai.com/v1/chat/completions \
          -H "Content-Type: application/json" \
          -H "Authorization: Bearer $OPENAI_API_KEY" \
          -d "$payload")
        
        # Check for API errors
        api_error=$(echo "$response" | jq -r '.error.message // ""')
        if [ -n "$api_error" ]; then
          echo "⚠️ OpenAI API error: $api_error"
          echo "feedback-text=*AI assessment encountered an error. Human reviewers will check code quality and tests.*" >> "$GITHUB_OUTPUT"
          echo "has-concerns=false" >> "$GITHUB_OUTPUT"
          echo "summary=AI assessment error" >> "$GITHUB_OUTPUT"
          exit 0
        fi
        
        # Extract feedback
        feedback=$(echo "$response" | jq -r '.choices[0].message.content // ""')
        
        if [ -z "$feedback" ]; then
          echo "⚠️ Empty response from AI"
          echo "feedback-text=*AI assessment returned no feedback. Human reviewers will check code quality and tests.*" >> "$GITHUB_OUTPUT"
          echo "has-concerns=false" >> "$GITHUB_OUTPUT"
          echo "summary=AI assessment empty" >> "$GITHUB_OUTPUT"
          exit 0
        fi
        
        echo "✅ AI assessment complete"
        
        # Detect if there are concerns (look for warning keywords)
        has_concerns=false
        if echo "$feedback" | grep -qiE "(concern|issue|problem|bug|missing|should|must|need to)"; then
          has_concerns=true
        fi
        
        # Create summary (first sentence)
        summary=$(echo "$feedback" | head -n 1 | cut -d'.' -f1)
        
        echo "feedback-text=$feedback" >> "$GITHUB_OUTPUT"
        echo "has-concerns=$has_concerns" >> "$GITHUB_OUTPUT"
        echo "summary=$summary" >> "$GITHUB_OUTPUT"
        
        echo "Assessment summary: $summary"

